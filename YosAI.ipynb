{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YosAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZaneZaiontz/YosAI/blob/main/YosAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEYA9jFmRG6t"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhciIDZgwhlQ"
      },
      "source": [
        "Opening and reading in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMoxvg3NrBei"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import random as rand\n",
        "\n",
        "def readFiles():\n",
        "    file = open('./zhaoClean.txt', 'r')\n",
        "    lines = file.readlines()\n",
        "    file.close()\n",
        "    file = open('./zhaoClean.txt', 'r')\n",
        "    return file.read().lower(), lines\n",
        "\n",
        "text, lines = readFiles()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmBIleLGq4ia"
      },
      "source": [
        "Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIYw88NhPTrt"
      },
      "source": [
        "# training length\n",
        "sequenceLen = 64\n",
        "vocab = sorted(set(text))\n",
        "# Unique ID \n",
        "charToInt = {j:i for i, j in enumerate(vocab)}\n",
        "intToChar = np.array(vocab)\n",
        "\n",
        "examples_per_epoch = len(text)//(sequenceLen+1)\n",
        "tmpData = np.array([charToInt[i] for i in text])\n",
        "charData = tf.data.Dataset.from_tensor_slices(tmpData)\n",
        "sequences = charData.batch(sequenceLen+1, drop_remainder=True)\n",
        "\n",
        "def splitInput(chunk):  \n",
        "    inText = chunk[:-1]  \n",
        "    toText = chunk[1:] \n",
        "    return inText, toText \n",
        "\n",
        "dataset = sequences.map(splitInput)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5U2foZxNDwT"
      },
      "source": [
        "Build Model/Shuffle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBJW_puxaqAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4651a554-9462-4e71-d1cd-65b6f633aa57"
      },
      "source": [
        "sizeOfEmbed = 256\n",
        "sizeOfRNN = 1024\n",
        "sizeOfBuff = 10000\n",
        "sizeofBatch = 64\n",
        "sizeOfVocab = len(vocab)\n",
        "\n",
        "data = dataset.shuffle(sizeOfBuff).batch(sizeofBatch, drop_remainder=True)\n",
        "def buildTheModel(vocabSize, embedSize, rnnSize, batchSize):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocabSize, embedSize, batch_input_shape=[batchSize, None]),\n",
        "    tf.keras.layers.LSTM(rnnSize, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocabSize)])\n",
        "  return model\n",
        "model = buildTheModel(sizeOfVocab, sizeOfEmbed, sizeOfRNN, sizeofBatch)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           15360     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 60)            61500     \n",
            "=================================================================\n",
            "Total params: 5,323,836\n",
            "Trainable params: 5,323,836\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSJU9H7s7lEs"
      },
      "source": [
        "Compile/Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtsk91CZ8Txx"
      },
      "source": [
        "Checkpoints/Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6W0Awyz8UDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56ed2eb-9886-4f52-e610-d6406fe1ccdd"
      },
      "source": [
        "numOfEpochs = 128\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "checkLocation = './trainingData'\n",
        "checkPre = os.path.join(checkLocation, \"CKPT_{epoch}\")\n",
        "\n",
        "checkBack=tf.keras.callbacks.ModelCheckpoint(filepath=checkPre, save_weights_only=True)\n",
        "history = model.fit(data, epochs=numOfEpochs, callbacks=[checkBack])\n",
        "\n",
        "model = buildTheModel(sizeOfVocab, sizeOfEmbed, sizeOfRNN, batchSize=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkLocation))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "61/61 [==============================] - 6s 46ms/step - loss: 3.2995\n",
            "Epoch 2/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 2.6099\n",
            "Epoch 3/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 2.3769\n",
            "Epoch 4/128\n",
            "61/61 [==============================] - 3s 48ms/step - loss: 2.2355\n",
            "Epoch 5/128\n",
            "61/61 [==============================] - 3s 48ms/step - loss: 2.1223\n",
            "Epoch 6/128\n",
            "61/61 [==============================] - 3s 49ms/step - loss: 2.0281\n",
            "Epoch 7/128\n",
            "61/61 [==============================] - 3s 48ms/step - loss: 1.9486\n",
            "Epoch 8/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.8790\n",
            "Epoch 9/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.8154\n",
            "Epoch 10/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.7693\n",
            "Epoch 11/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.7047\n",
            "Epoch 12/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.6609\n",
            "Epoch 13/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.6252\n",
            "Epoch 14/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.5967\n",
            "Epoch 15/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.5633\n",
            "Epoch 16/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.5332\n",
            "Epoch 17/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.5014\n",
            "Epoch 18/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.4746\n",
            "Epoch 19/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.4534\n",
            "Epoch 20/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.4168\n",
            "Epoch 21/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.3984\n",
            "Epoch 22/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.3713\n",
            "Epoch 23/128\n",
            "61/61 [==============================] - 3s 45ms/step - loss: 1.3513\n",
            "Epoch 24/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.3342\n",
            "Epoch 25/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.2975\n",
            "Epoch 26/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.2734\n",
            "Epoch 27/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.2394\n",
            "Epoch 28/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.2187\n",
            "Epoch 29/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.1938\n",
            "Epoch 30/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.1595\n",
            "Epoch 31/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.1235\n",
            "Epoch 32/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.1002\n",
            "Epoch 33/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.0670\n",
            "Epoch 34/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.0366\n",
            "Epoch 35/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.0048\n",
            "Epoch 36/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.9691\n",
            "Epoch 37/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.9399\n",
            "Epoch 38/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.9024\n",
            "Epoch 39/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.8703\n",
            "Epoch 40/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.8384\n",
            "Epoch 41/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.8107\n",
            "Epoch 42/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.7843\n",
            "Epoch 43/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.7536\n",
            "Epoch 44/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.7228\n",
            "Epoch 45/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.6940\n",
            "Epoch 46/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.6654\n",
            "Epoch 47/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.6419\n",
            "Epoch 48/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.6178\n",
            "Epoch 49/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5971\n",
            "Epoch 50/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5784\n",
            "Epoch 51/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5596\n",
            "Epoch 52/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5418\n",
            "Epoch 53/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5227\n",
            "Epoch 54/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5071\n",
            "Epoch 55/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4907\n",
            "Epoch 56/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4815\n",
            "Epoch 57/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4650\n",
            "Epoch 58/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4567\n",
            "Epoch 59/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4482\n",
            "Epoch 60/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4348\n",
            "Epoch 61/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4292\n",
            "Epoch 62/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4241\n",
            "Epoch 63/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4140\n",
            "Epoch 64/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4102\n",
            "Epoch 65/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4056\n",
            "Epoch 66/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4006\n",
            "Epoch 67/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3879\n",
            "Epoch 68/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3861\n",
            "Epoch 69/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3793\n",
            "Epoch 70/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3765\n",
            "Epoch 71/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3704\n",
            "Epoch 72/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3673\n",
            "Epoch 73/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3660\n",
            "Epoch 74/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3614\n",
            "Epoch 75/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3574\n",
            "Epoch 76/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3578\n",
            "Epoch 77/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3517\n",
            "Epoch 78/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3470\n",
            "Epoch 79/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3439\n",
            "Epoch 80/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3409\n",
            "Epoch 81/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3409\n",
            "Epoch 82/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3359\n",
            "Epoch 83/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3318\n",
            "Epoch 84/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3300\n",
            "Epoch 85/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3317\n",
            "Epoch 86/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3277\n",
            "Epoch 87/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3240\n",
            "Epoch 88/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3245\n",
            "Epoch 89/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3195\n",
            "Epoch 90/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3205\n",
            "Epoch 91/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3163\n",
            "Epoch 92/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3124\n",
            "Epoch 93/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3147\n",
            "Epoch 94/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3139\n",
            "Epoch 95/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3112\n",
            "Epoch 96/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3071\n",
            "Epoch 97/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3119\n",
            "Epoch 98/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3047\n",
            "Epoch 99/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3023\n",
            "Epoch 100/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2993\n",
            "Epoch 101/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2990\n",
            "Epoch 102/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2998\n",
            "Epoch 103/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2959\n",
            "Epoch 104/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2986\n",
            "Epoch 105/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2944\n",
            "Epoch 106/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2955\n",
            "Epoch 107/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2929\n",
            "Epoch 108/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2905\n",
            "Epoch 109/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2899\n",
            "Epoch 110/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2898\n",
            "Epoch 111/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2872\n",
            "Epoch 112/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2858\n",
            "Epoch 113/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2856\n",
            "Epoch 114/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2856\n",
            "Epoch 115/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2806\n",
            "Epoch 116/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2843\n",
            "Epoch 117/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2803\n",
            "Epoch 118/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2790\n",
            "Epoch 119/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2752\n",
            "Epoch 120/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2772\n",
            "Epoch 121/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2780\n",
            "Epoch 122/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2737\n",
            "Epoch 123/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2740\n",
            "Epoch 124/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2732\n",
            "Epoch 125/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2717\n",
            "Epoch 126/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2712\n",
            "Epoch 127/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2756\n",
            "Epoch 128/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5BT2rdl8oNt"
      },
      "source": [
        "Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2VXDtFK8sa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb53336-a92b-4fe3-fe16-8a38ea2ad23d"
      },
      "source": [
        "# createPoem generates 3 lines of words\n",
        "def createPoem(model):\n",
        "  newL = 0\n",
        "\n",
        "  startInt = (rand.randint(0, len(lines)))/3\n",
        "  startLine = lines[int(startInt)]\n",
        "  startLine = startLine.lower()\n",
        "\n",
        "  checkInput = [charToInt[s] for s in startLine]\n",
        "  checkInput = tf.expand_dims(checkInput, 0)\n",
        "  generatedWord = []\n",
        "  model.reset_states()\n",
        "\n",
        "  while newL < 3:\n",
        "      predictions = model(checkInput)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions\n",
        "      idPredict = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      checkInput = tf.expand_dims([idPredict], 0)\n",
        "      if (intToChar[idPredict] == '\\n'):\n",
        "        newL += 1\n",
        "      if (newL >= 3):\n",
        "        generatedWord.append(intToChar[idPredict])\n",
        "        break\n",
        "      generatedWord.append(intToChar[idPredict])\n",
        "  return (''.join(generatedWord))\n",
        "\n",
        "print(createPoem(model))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storm cloudside\n",
            "the cliff hing\n",
            "in gray murms\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}