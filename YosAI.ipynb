{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YosAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZaneZaiontz/YosAI/blob/main/YosAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhciIDZgwhlQ"
      },
      "source": [
        "Imports/Opening and reading in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMoxvg3NrBei"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import random as rand\n",
        "import numpy as np\n",
        "import keras, os\n",
        "\n",
        "def readFiles():\n",
        "    file = open('./zhaoClean.txt', 'r')\n",
        "    lines = file.readlines()\n",
        "    file.close()\n",
        "    file = open('./zhaoClean.txt', 'r')\n",
        "    return file.read().lower(), lines\n",
        "\n",
        "text, lines = readFiles()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmBIleLGq4ia"
      },
      "source": [
        "Character Encoding/Sequence mapping (setting up future predictive modelling)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIYw88NhPTrt"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Unique ID \n",
        "charToInt = {j:i for i, j in enumerate(vocab)}\n",
        "intToChar = np.array(vocab)\n",
        "\n",
        "tmpData = np.array([charToInt[i] for i in text])\n",
        "charData = tf.data.Dataset.from_tensor_slices(tmpData)\n",
        "# training char length\n",
        "sequenceLen = 64\n",
        "sequences = charData.batch(sequenceLen+1, drop_remainder=True)\n",
        "\n",
        "def splitInput(chunk):  \n",
        "    inText = chunk[:-1]  \n",
        "    toText = chunk[1:] \n",
        "    return inText, toText \n",
        "\n",
        "dataset = sequences.map(splitInput)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5U2foZxNDwT"
      },
      "source": [
        "Build Model/Shuffle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBJW_puxaqAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f168829c-5336-4602-c870-2c8dab726561"
      },
      "source": [
        "# Training data\n",
        "sizeOfEmbed = 256\n",
        "sizeOfRNN = 1024\n",
        "sizeOfBuff = 10000\n",
        "sizeofBatch = 64\n",
        "sizeOfVocab = len(vocab)\n",
        "\n",
        "data = dataset.shuffle(sizeOfBuff).batch(sizeofBatch, drop_remainder=True)\n",
        "def buildModel(vocabSize, embedSize, rnnSize, batchSize):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocabSize, embedSize, batch_input_shape=[batchSize, None]),\n",
        "    tf.keras.layers.LSTM(rnnSize, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocabSize)])\n",
        "  return model\n",
        "model = buildModel(sizeOfVocab, sizeOfEmbed, sizeOfRNN, sizeofBatch)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           15360     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 60)            61500     \n",
            "=================================================================\n",
            "Total params: 5,323,836\n",
            "Trainable params: 5,323,836\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSJU9H7s7lEs"
      },
      "source": [
        "Compile/Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtsk91CZ8Txx"
      },
      "source": [
        "Checkpoints/Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6W0Awyz8UDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395665ff-bd72-4d5a-c209-a3d0a5535d11"
      },
      "source": [
        "numOfEpochs = 128\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "checkLocation = './trainingData'\n",
        "checkPre = os.path.join(checkLocation, \"CKPT_{epoch}\")\n",
        "\n",
        "checkBack=tf.keras.callbacks.ModelCheckpoint(filepath=checkPre, save_weights_only=True)\n",
        "history = model.fit(data, epochs=numOfEpochs, callbacks=[checkBack])\n",
        "\n",
        "model = buildModel(sizeOfVocab, sizeOfEmbed, sizeOfRNN, batchSize=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkLocation))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "61/61 [==============================] - 6s 47ms/step - loss: 3.2538\n",
            "Epoch 2/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 2.5852\n",
            "Epoch 3/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 2.3512\n",
            "Epoch 4/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 2.2033\n",
            "Epoch 5/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 2.0959\n",
            "Epoch 6/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.9915\n",
            "Epoch 7/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.9113\n",
            "Epoch 8/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.8439\n",
            "Epoch 9/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.7726\n",
            "Epoch 10/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.7175\n",
            "Epoch 11/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.6646\n",
            "Epoch 12/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.6195\n",
            "Epoch 13/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.5830\n",
            "Epoch 14/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.5505\n",
            "Epoch 15/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.5116\n",
            "Epoch 16/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.4890\n",
            "Epoch 17/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.4675\n",
            "Epoch 18/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.4221\n",
            "Epoch 19/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.3985\n",
            "Epoch 20/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.3711\n",
            "Epoch 21/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.3417\n",
            "Epoch 22/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.3106\n",
            "Epoch 23/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.2843\n",
            "Epoch 24/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.2526\n",
            "Epoch 25/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.2267\n",
            "Epoch 26/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.1923\n",
            "Epoch 27/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.1610\n",
            "Epoch 28/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 1.1315\n",
            "Epoch 29/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.0885\n",
            "Epoch 30/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.0554\n",
            "Epoch 31/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 1.0242\n",
            "Epoch 32/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.9868\n",
            "Epoch 33/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.9480\n",
            "Epoch 34/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.9092\n",
            "Epoch 35/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.8729\n",
            "Epoch 36/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.8393\n",
            "Epoch 37/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.8054\n",
            "Epoch 38/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.7708\n",
            "Epoch 39/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.7372\n",
            "Epoch 40/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.7116\n",
            "Epoch 41/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.6773\n",
            "Epoch 42/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.6510\n",
            "Epoch 43/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.6204\n",
            "Epoch 44/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5963\n",
            "Epoch 45/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.5726\n",
            "Epoch 46/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5583\n",
            "Epoch 47/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5338\n",
            "Epoch 48/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.5152\n",
            "Epoch 49/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4986\n",
            "Epoch 50/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4852\n",
            "Epoch 51/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4752\n",
            "Epoch 52/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4599\n",
            "Epoch 53/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.4498\n",
            "Epoch 54/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4379\n",
            "Epoch 55/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.4326\n",
            "Epoch 56/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.4220\n",
            "Epoch 57/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4132\n",
            "Epoch 58/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.4053\n",
            "Epoch 59/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.4028\n",
            "Epoch 60/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3915\n",
            "Epoch 61/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3823\n",
            "Epoch 62/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3837\n",
            "Epoch 63/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3765\n",
            "Epoch 64/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3746\n",
            "Epoch 65/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3694\n",
            "Epoch 66/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3642\n",
            "Epoch 67/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3593\n",
            "Epoch 68/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3593\n",
            "Epoch 69/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3539\n",
            "Epoch 70/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3471\n",
            "Epoch 71/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3471\n",
            "Epoch 72/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3445\n",
            "Epoch 73/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3394\n",
            "Epoch 74/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3374\n",
            "Epoch 75/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3322\n",
            "Epoch 76/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3289\n",
            "Epoch 77/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3277\n",
            "Epoch 78/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3256\n",
            "Epoch 79/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3263\n",
            "Epoch 80/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3214\n",
            "Epoch 81/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3206\n",
            "Epoch 82/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3189\n",
            "Epoch 83/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3145\n",
            "Epoch 84/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3140\n",
            "Epoch 85/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3132\n",
            "Epoch 86/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3100\n",
            "Epoch 87/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3098\n",
            "Epoch 88/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.3083\n",
            "Epoch 89/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3058\n",
            "Epoch 90/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3059\n",
            "Epoch 91/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.3001\n",
            "Epoch 92/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.2971\n",
            "Epoch 93/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2999\n",
            "Epoch 94/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2992\n",
            "Epoch 95/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2943\n",
            "Epoch 96/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2913\n",
            "Epoch 97/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2920\n",
            "Epoch 98/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2899\n",
            "Epoch 99/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2917\n",
            "Epoch 100/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2897\n",
            "Epoch 101/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2885\n",
            "Epoch 102/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2847\n",
            "Epoch 103/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2831\n",
            "Epoch 104/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2817\n",
            "Epoch 105/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2841\n",
            "Epoch 106/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2797\n",
            "Epoch 107/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2779\n",
            "Epoch 108/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2787\n",
            "Epoch 109/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2769\n",
            "Epoch 110/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2752\n",
            "Epoch 111/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2755\n",
            "Epoch 112/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2765\n",
            "Epoch 113/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2715\n",
            "Epoch 114/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2735\n",
            "Epoch 115/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2734\n",
            "Epoch 116/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.2721\n",
            "Epoch 117/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2671\n",
            "Epoch 118/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2724\n",
            "Epoch 119/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2689\n",
            "Epoch 120/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2653\n",
            "Epoch 121/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2659\n",
            "Epoch 122/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2643\n",
            "Epoch 123/128\n",
            "61/61 [==============================] - 3s 47ms/step - loss: 0.2600\n",
            "Epoch 124/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2618\n",
            "Epoch 125/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2622\n",
            "Epoch 126/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2635\n",
            "Epoch 127/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2579\n",
            "Epoch 128/128\n",
            "61/61 [==============================] - 3s 46ms/step - loss: 0.2566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5BT2rdl8oNt"
      },
      "source": [
        "Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2VXDtFK8sa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db66101d-92ad-4e77-afc7-d2be3efa62fc"
      },
      "source": [
        "# createPoem generates 3 lines of words\n",
        "def createPoem(model):\n",
        "  newL = 0\n",
        "\n",
        "  startInt = (rand.randint(0, len(lines)))/3\n",
        "  startLine = lines[int(startInt)]\n",
        "  startLine = startLine.lower()\n",
        "\n",
        "  checkInput = [charToInt[s] for s in startLine]\n",
        "  checkInput = tf.expand_dims(checkInput, 0)\n",
        "  generatedWord = []\n",
        "  model.reset_states()\n",
        "\n",
        "  # For haiku format\n",
        "  while newL < 3:\n",
        "      # Predict based off previous sequence\n",
        "      predictions = model(checkInput)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions\n",
        "      idPredict = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      checkInput = tf.expand_dims([idPredict], 0)\n",
        "      if (intToChar[idPredict] == '\\n'):\n",
        "        newL += 1\n",
        "      if (newL >= 3):\n",
        "        generatedWord.append(intToChar[idPredict])\n",
        "        break\n",
        "      generatedWord.append(intToChar[idPredict])\n",
        "  return (''.join(generatedWord))\n",
        "\n",
        "print(createPoem(model))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree\n",
            "sombine colors\n",
            "in the morning ed empty\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}